# `longtext20260110`  
**长文本处理小程序**  

- 自带图形处理界面（GUI）  
- 使用简单，支持本地大模型与网络大模型  
- 字数无上限  
- 采用 **拆分 → 批处理 → 合并** 三模块架构
- 为降低程序复杂度，便于后期升级，三模块单独架构，需单独手动操作。先使用拆分模块拆分文件，然后使用批处理模块批处理文件夹里的文件，满意后再使用合并模块合并。  
- 仿“酒馆”设计风格  
- 支持加载提示词、预设、正则规则（单此仅支持加入一个正则，如有多个正则需要加入请自行合并后加入）  
- ⚠️ 使用本程序需自备大模型 API  

---

## 模块1：文本分块工具使用说明

### 功能简介
本工具提供两种文本分块模式，适用于大语言模型（LLM）输入预处理等场景：

- **模式 A（Token 精细分块）**  
  - 按指定 Token 数量切分  
  - 自动保留句子/段落边界  
  - 支持重叠，避免语义割裂  

- **模式 B（章节分块）**  
  - 基于章节标题智能识别  
  - 将完整章节组合成块，确保逻辑完整性  

---

### 使用方法

#### 1. 运行程序
- 双击运行脚本，弹出图形界面（GUI）  
- 首次使用请安装依赖：  
  ```bash
  pip install tiktoken
  ```

#### 2. 选择文件
- 点击“选择文件”按钮，选取 `.txt` 文本文件  
- 程序自动尝试多种编码（UTF-8、GBK 等）读取  
- 显示总 Token 数（基于 OpenAI 的 `cl100k_base` 编码）

#### 3. 选择分块模式
| 模式 | 适用场景 |
|------|--------|
| **模式 A** | 无明确章节结构的长文（论文、报告、网页内容） |
| **模式 B** | 有清晰章节划分的文本（小说、教材、技术文档） |

#### 4. 设置参数

##### 模式 A 参数：
- **目标 Token 大小**：每块目标长度（默认 `2500`）  
- **重叠率**：相邻块重叠比例（默认 `0.05`，即 5%）  
- **最小区块比例**：防止过小块的阈值（默认 `0.2`，即 ≥20% 目标大小）

##### 模式 B 参数：
- **最大 Token 上限**：每个输出块的最大 Token 数（默认 `5000`）

#### 5. 开始分块
- 点击“开始分块”按钮  
- 输出目录：`OUT/时间_文件名/`（例如 `OUT/20260110_report/`）  
- 输出内容：
  - 所有分块文本文件（`.txt`）
  - （仅模式 A）`metadata/` 文件夹中的 JSON 元数据（记录位置、Token 数等）

#### 6. 完成提示
- 成功：弹窗显示分块数量及保存路径  
- 失败：显示具体错误原因（如编码问题、参数无效等）

---

### 注意事项
- 输入文件必须为 **纯文本（`.txt`）**，不支持 Word/PDF  
- Token 计算基于 **OpenAI 的 `cl100k_base` 编码**（与 GPT-3.5/4 一致）  
- 模式 B 依赖 **正则匹配识别章节标题**，非标准格式可能失效  
- 输出文件统一使用 **UTF-8 编码**  

✅ 适用于：本地部署、批量预处理、RAG 数据准备等场景

---

## 模块2：文件批处理模块使用说明

### 功能简介
一款功能全面的大模型文本处理平台，支持：

- ✅ **API 配置管理**：适配 OpenAI 兼容接口（Ollama、vLLM、本地模型等）  
- 📁 **批量处理 + 断点续传**：自动跳过已处理文件，支持失败重试（最多 3 次）  
- 💬 **提示词 & 系统预设管理**：可保存/加载模板  
- 🔧 **正则后处理**：自定义规则清洗输出（如清理空行、标准化格式）  
- 📊 **实时进程监控**：进度条 + 文件状态（✅成功 / ❌失败 / 🔄处理中）  
- 📝 **完整日志记录**：每个任务生成独立日志，便于调试

---

### 使用流程

#### 第一步：配置 API（首次运行必做）
1. 启动程序 → 自动打开 “大模型API配置”窗口  
2. 填写：
   - **API 地址**（默认：`http://localhost:11434/api/generate`，适用于 Ollama）  
   - **API 密钥**（如无需留空）  
   - **超时时间**（建议 ≥180 秒）  
   - **批量间隔**（默认 3 秒，防请求过快）  
3. 点击 “测试连接” → 自动获取可用模型列表  
4. 从下拉框选择目标模型 → “保存配置”

> ⚠️ 注意：程序会自动将 `/api/generate` 转换为兼容 OpenAI 的 `/v1/chat/completions` 接口。

---

#### 第二步：主界面操作

##### 1. 选择处理模式
- **单文件模式**：处理单个 `.txt` 文件  
- **批量文件夹模式**：处理整个文件夹内所有 `.txt` 文件（支持断点续传）

##### 2. 设置处理逻辑

| 区域 | 说明 |
|------|------|
| **输入提示词** | 主要指令（如“润色以下文本”），支持多行编辑 |
| **系统预设（可选）** | 设定 AI 角色（如“你是一位专业编辑”） |
| **正则后处理规则** | 每行格式：`pattern → replacement`（支持多行） |

💡 所有区域均支持 **保存/加载**（`.prompt` / `.preset` / `.regex`/ `.json` 文件）

##### 3. 开始处理
- 点击 “开始处理”  
- 输出目录：`OUT/时间戳_任务名/`（如 `OUT/20260110_143022_小说批任务/`）  
- 输出内容：
  - 处理结果文件（`*_out.txt` 或 `*_processed.txt`）  
  - 详细日志（`*_log.txt`）  
  - 失败文件生成 `_error.txt` 占位符

---

### 高级特性

- **🔁 断点续传**  
  批量模式下，再次运行自动跳过已成功文件，仅处理剩余或失败项

- **🔄 自动重试机制**  
  单个文件失败 → 自动重试最多 3 次（间隔 10 秒）→ 仍失败则生成错误报告

- **📂 输出结构示例**
  ```
  OUT/
  └── 20260110_143022_小说批任务/
      ├── chapter01_processed.txt
      ├── chapter02_error.txt
      └── batch_log.txt          # 完整处理日志
  ```

- **⚙️ 快捷操作**
  - 保存当前设置 → 导出 `.profile` 文件（含提示词+预设+正则）  
  - 重新配置 API → 随时修改模型或密钥  
  - 自动打开结果文件夹（Windows 下处理完成后弹窗并打开）

---

### 注意事项
- 输入文件必须为 **UTF-8/GBK 等常见编码的纯文本（`.txt`）**  
- 正则规则中的 `\n`、`\t` 需写为字面量（程序会自动转义）  
- 若使用 Ollama，请确保模型已通过 `ollama pull` 下载  
- 网络不稳定时，建议增大 **超时时间** 和 **批量间隔**

✅ 适用于：内容润色、格式转换、数据清洗、批量摘要、本地 LLM 自动化处理等场景

---

## 模块3：一键合并拆分文本模块操作说明

### 功能
自动将符合命名规则的多个 `.txt` 拆分文件按原始来源合并为一个完整文档。

---

### 文件命名要求
文件名必须严格遵循格式：  
```
AAA_chunk_NBBB.txt
```
- `AAA`：主文件名（任意非空字符串）  
- `N`：段落序号（纯数字，如 `1`, `2`, `05`…）  
- `BBB`：可选后缀（可为空，用于区分不同批次）

✅ 示例：
- `report_chunk_1.txt`  
- `novel_chunk_05_end.txt`

---

### 使用步骤

1. **运行程序**：双击执行脚本  
2. **选择文件夹**：在弹出窗口中选择包含拆分文件的文件夹  
3. **自动生成**：
   - 程序按 `AAA` 分组  
   - 同一组内所有 `_chunk_N` 文件按数字序号排序合并  
   - 输出文件命名为：`AAA_zong_out.txt`（如 `report_zong_out.txt`）  
4. **查看结果**：
   - 合并完成后自动打开结果所在文件夹  
   - 每个段落以 `【段落001】` 标题开头  
   - 段落间用 `----` 分隔

---

### 注意事项
- 仅处理 `.txt` 文件，其他格式自动忽略  
- 若文件读取失败，对应段落标记为 `[读取失败]`  
- 同一 `AAA` 组内的文件 **必须连续编号**（不要求从 1 开始）
